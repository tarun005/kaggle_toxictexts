{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(line):\n",
    "\tline = line.lower()\n",
    "\treturn re.split('\\W+|_' , line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "\n",
    "\tdef __init__(self):\n",
    "\n",
    "\t\tself.word_to_idx = {}\n",
    "\t\tself.idx_to_word = {}\n",
    "\t\tself.word_freq = defaultdict(int)\n",
    "\t\tself.total_words = 0 #float(sum(self.word_data.values()))\n",
    "\t\tself.unknown = unknown_string\n",
    "\t\tself.add_word(self.unknown , count=0)\n",
    "\n",
    "\tdef add_word(self ,word , count=1):\n",
    "\t\tif word not in word_to_idx:\n",
    "\t\t\tindex = len(self.word_to_idx)\n",
    "\t\t\tself.word_to_idx[word] = index\n",
    "\t\t\tself.idx_to_word[index] = word\n",
    "\t\tself.word_freq[word] += count\n",
    "\n",
    "\tdef construct(self, words_list , replace_digits=True):\n",
    "\t\tfor word,count in words_list.items():\n",
    "\t\t\tif any([c.isdigit() for c in word]) and replace_digits:\n",
    "\t\t\t\tword = self.unknown\n",
    "\t\t\tself.add_word(word , count)\n",
    "\t\ttotal_words = float(sum(self.word_freq.values()))\n",
    "\t\tprint('Total {} words with {} uniques'.format(total_words , len(self.word_freq)) )\n",
    "\n",
    "\tdef encode(self, word):\n",
    "\t\tif word not in word_to_idx:\n",
    "\t\t\tword = self.unknown\n",
    "\t\treturn word_to_idx[word]\n",
    "\n",
    "\tdef decode(self , index):\n",
    "\t\treturn idx_to_word[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-6f7458795187>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6f7458795187>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    for idx in num_batches\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_batches(X, y=None , batch_size=1 , augment_method='pad' , common_size=10):\n",
    "\n",
    "\tnum_batches = int(len(X)/batch_size)\n",
    "\taugment_method = augment_method.lower()\n",
    "\n",
    "\t## Sort the entries in data with length of sentences to reduce computation time.\n",
    "\tsort_idx = [val[0] for val in sorted(enumerate(X) , key = lambda K : len(get_words(K[1])))]\n",
    "\tX = X[sort_idx]\n",
    "\ty = y[sort_idx]\n",
    "\n",
    "\tif y is None:\n",
    "\t\tget_labels = False\n",
    "\n",
    "\tX_data = []\n",
    "\ty_data = []\n",
    "\tseq_lengths = []\n",
    "\n",
    "\tfor idx in num_batches\n",
    "\t\tsents = np.reshape(X.loc[idx*batch_size:(idx+1)*batch_size] , [-1,1])\n",
    "\n",
    "\t\tif get_labels:\n",
    "\t\t\tlabels = y.loc[idx*batch_size:(idx+1)*batch_size , :]\n",
    "\t\t\ty_data.append(labels)\n",
    "\n",
    "\t\tbatch_words = [get_words(sent) for sent in sents]\n",
    "\t\tseq_lengths.append([len(batch) for batch in batch_words])\n",
    "\t\tmodified_words = []\n",
    "\n",
    "\t\tif augment_method == 'pad': ## Match the maximum\n",
    "\t\t\tmax_len = max([len(batch) for batch in batch_words])\n",
    "\t\t\tmodified_words = []\n",
    "\t\t\tfor batch in batch_words:\n",
    "\t\t\t\tlen_batch = len(batch)\n",
    "\t\t\t\tall_words = batch + [unknown_string]*(max_len - len_batch)\n",
    "\t\t\t\tmodified_words.append(all_words)\n",
    "\n",
    "\t\telif augment_method == 'adjust': ## Adjust to a fixed length\n",
    "\t\t\tfixed_len = common_size\n",
    "\t\t\tmodified_words = []\n",
    "\t\t\tfor batch in batch_words:\n",
    "\t\t\t\tlen_batch = len(batch)\n",
    "\t\t\t\tif len_batch < common_size:\n",
    "\t\t\t\t\tall_words = batch + [unknown_string]*(fixed_len - len_batch)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tall_words = batch[:common_size]\n",
    "\t\t\t\tmodified_words.append(all_words)\n",
    "\n",
    "\t\telif augment_method == 'truncate': ## Match the minimum\n",
    "\t\t\tmin_len = min([len(batch) for batch in batch_words])\n",
    "\t\t\tmodified_words = []\n",
    "\t\t\tfor batch in batch_words:\n",
    "\t\t\t\tall_words = batch[:common_size]\n",
    "\t\t\t\tmodified_words.append(all_words)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Augment Method not specified or not understood. Should be one of 'pad' , 'adjust' or 'truncate'\")\n",
    "\n",
    "\t\tX_data.append(modified_words)\n",
    "\n",
    "\tif get_labels:\n",
    "\t\treturn X_data, seq_lengths , y_data\n",
    "\treturn X_data , seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel():\n",
    "\n",
    "\tdef load_data(self , datafile):\n",
    "\n",
    "\t\tdataset = pd.read_csv('train.csv')\n",
    "\t\ttext = 'comment_text'\n",
    "\t\tself.X = dataset[text].values\n",
    "\n",
    "\t\tif self.test_mode:\n",
    "\t\t\tself.X_test = self.X\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\tlabels = ['toxic', 'severe_toxic', 'obscene' , 'threat', 'insult', 'identity_hate']\n",
    "\t\tself.y = dataset[labels].values\n",
    "\t\tself.X_train , self.X_val , self.y_train , self.y_val = train_test_split(self.X , self.y, test_size=0.1, random_state=1234)\n",
    "\n",
    "\t\t## Build the vocabulary using the train data.\n",
    "\t\tself.vocab = Vocab()\n",
    "\t\ttrain_sents = [get_words(line) for line in self.X_train]\n",
    "\t\tall_words = list(itertools.chain.from_iterable(train_sents))\n",
    "\t\tunique_words , n_occs = np.unique(all_words , return_counts=True) ## Get unique tokens\n",
    "\t\twords_freq_dictionary = dict(zip(unique_words , n_occs))\n",
    "\t\tself.vocab.construct(words_freq_dictionary)\n",
    "\n",
    "\tdef define_weights(self):\n",
    "\t\tembed_size = self.config.embed_size\n",
    "\t\thidden_size = self.config.hidden_size\n",
    "\t\tlabel_size = self.config.label_size\n",
    "\t\tvocab_size = len(self.vocab)\n",
    "\n",
    "\t\t## Declare weights and placeholders\n",
    "\t\twith tf.variable_scope(\"Embeddings\" , initializer = tf.contrib.layers.xavier_initializer) as scope:\n",
    "\t\t\tembedding = tf.get_variable(\"Embeds\" , [vocab_size , embed_size])\n",
    "\n",
    "\t\twith tf.variable_scope(\"Output\" , initializer = tf.contrib.layers.xavier_initializer) as scope:\n",
    "\t\t\tW_o = tf.get_variable(\"Weight\" , [hidden_size , label_size])\n",
    "\t\t\tb_o = tf.get_variable(\"Bias\" , [label_size])\n",
    "\t\t\tself.wo_l2loss = tf.nn.l2_loss(W_o)\n",
    "\n",
    "\t\t## Define the placeholders\n",
    "\t\tself.input_placeholder = tf.placeholder(tf.int32 , [None , None])\n",
    "\t\tself.label_placeholder = tf.placeholder(tf.int32 , [None , label_size])\n",
    "\t\tself.sequence_length_placeholder = tf.placeholder(tf.int32 , [None])\n",
    "\n",
    "\t\tself.cellstate_placeholder = tf.placeholder(tf.float32 , [None , hidden_size])\n",
    "\t\tself.hiddenstate_placeholder = tf.placeholder(tf.float32 , [None , hidden_size])\n",
    "\n",
    "\tdef input_embeddings(self):\n",
    "\n",
    "\t\twith tf.variable_scope(\"Embeddings\"):\n",
    "\t\t\tembedding = tf.get_variable(\"Embeds\")\n",
    "\n",
    "\t\tinput_vectors = tf.nn.embedding_lookup(embedding , self.input_placeholder)\n",
    "\t\tnum_splits = input_vectors.get_shape().as_list()[1]\n",
    "\t\tinput_series = [tf.squeeze(input_step_mat,axis=1) for input_step_mat in tf.split(input_vectors , num_splits, axis=1)]\n",
    "\n",
    "\t\treturn input_series\n",
    "\n",
    "\tdef LSTM_module(self , input_series):\n",
    "\n",
    "\t\tstate_tuple = tf.contrib.rnn.LSTMStateTuple(self.cellstate_placeholder , self.hiddenstate_placeholder)\n",
    "\t\tLSTMcell = tf.contrib.rnn.BasicLSTMCell (num_units = self.config.hidden_size , state_is_tuple=True)\n",
    "\t\thidden_states , last_state = tf.nn.static_rnn(LSTMcell , input_series ,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   state_tuple,\tsequence_length=self.sequence_length_placeholder)\n",
    "\t\tlast_cellstate , last_hiddenstate = last_state\n",
    "\n",
    "\t\twith tf.variable_scope(\"Output\"):\n",
    "\t\t\tW_o = tf.get_variable(\"Weight\")\n",
    "\t\t\tb_o = tf.get_variable(\"Bias\")\n",
    "\n",
    "\t\t\toutput = tf.matmul(W_o , last_hiddenstate) + b_o\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "\tdef calculate_loss(self , output):\n",
    "\n",
    "\t\tlabels = self.label_placeholder\n",
    "\n",
    "\t\tlog_loss = tf.reduce_mean(tf.multiply(labels , tf.log_sigmoid(output)) +\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t tf.multiply((1-labels) , tf.log_sigmoid(-1*output)))\n",
    "\t\tl2_loss = 0\n",
    "\t\tfor weights in tf.trainable_variables():\n",
    "\t\t\tif (\"Bias\" not in weights.name) and (\"Embeddings\" not in weights.name): \n",
    "\t\t\t\tl2_loss += self.config.l2_loss(weights)\n",
    "\n",
    "\t\tloss = log_loss #+ l2_loss\n",
    "\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef training_operation(self , loss):\n",
    "\t\tself.train_op = tf.train.AdamOptimizer(learning_rate=self.config.lr).minimize(loss)\n",
    "\n",
    "\tdef __init__(self , config , datafile , test=False):\n",
    "\t\tself.config = config\n",
    "\t\tself.test_mode = test\n",
    "\t\tself.load_data(datafile)\n",
    "\t\tself.define_weights()\n",
    "\t\tinput_series = self.input_embeddings()\n",
    "\t\toutput = self.LSTM_module(input_series)\n",
    "\t\tself.loss = self.calculate_loss(output)\n",
    "\t\tself.training_operation(loss)\n",
    "\n",
    "\t\tself.pred = tf.cast(tf.greater(tf.nn.sigmoid(output) , 0.5) , tf.int32)\n",
    "\n",
    "\tdef build_feeddict(self, X, seq_len, y=None):\n",
    "\n",
    "\t\tX_input = []\n",
    "\n",
    "\t\tfor sent in X:\n",
    "\t\t\tsent_tokens = []\n",
    "\t\t\tfor word in sent:\n",
    "\t\t\t\tsent_tokens.append(self.vocab.encode(word))\n",
    "\t\t\tX_input.append(sent_tokens)\n",
    "\n",
    "\t\tX_input = np.array(X_input)\n",
    "\t\tassert(X_input.shape[0] == self.config.batch_size)\n",
    "\n",
    "\t\ty_input = y\n",
    "\t\tseq_len_input = np.reshape(np.array(seq_len) , [-1])\n",
    "\t\tassert(len(seq_len_input) == self.config.batch_size)\n",
    "\n",
    "\t\tfeed = {self.input_placeholder : X_input,\n",
    "\t\t\t\tself.sequence_length_placeholder : seq_len_input,\n",
    "\t\t\t\tself.label_placeholder : y_input,\n",
    "\t\t\t\tself.cellstate_placeholder : np.zeros([self.config.batch_size , self.config.hidden_size]),\n",
    "\t\t\t\tself.hiddenstate_placeholder : np.zeros([self.config.batch_size , self.config.hidden_size]),\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\treturn feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels , predictions , classwise=True):\n",
    "\n",
    "\tif classwise:\n",
    "\t\tclass_wise_acc = np.mean(np.equal(labels , predictions) , axis=0, keepdims=True)\n",
    "\t\treturn class_wise_acc\n",
    "\telse:\n",
    "\t\tacc = np.mean(np.equal(labels , predictions))\n",
    "\t\treturn acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(sess , model, verbose=True):\n",
    "\n",
    "\tepoch_train_pred = []\n",
    "\tepoch_train_label = []\n",
    "\tepoch_train_loss = []\n",
    "\n",
    "\tepoch_val_pred = []\n",
    "\tepoch_val_label = []\n",
    "\tepoch_val_loss = []\n",
    "\n",
    "\tstep = 0\n",
    "\n",
    "\tfor train_X, train_seq_len, train_y in get_batches(model.X_train , model.y_train, model.config.batch_size):\n",
    "\t\tfeed = model.build_feeddict(train_X , train_seq_len, train_y) \n",
    "\t\tclass_pred , batch_loss , _ = sess.run([model.pred , model.loss, model.train_op] , feed_dict=feed)\n",
    "\t\tepoch_train_pred.append(class_pred)\n",
    "\t\tepoch_train_label.append(train_y)\n",
    "\t\tepoch_train_loss.append(batch_loss)\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tsys.stdout.write('\\r{} / {} :    loss = {}'.format(step*self.config.batch_size, len(model.X_train), np.mean(epoch_train_loss)))\n",
    "\t\t\tsys.stdout.flush()\n",
    "\n",
    "\t\tstep += 1\n",
    "\n",
    "\tfor val_X , val_seq_len, val_y in get_batches(model.X_val, model.y_val, model.config.batch_size):\n",
    "\t\tfeed = model.build_feeddict(val_X , val_seq_len, val_y)\n",
    "\t\tval_preds , val_loss = sess.run([model.pred , model.loss] , feed_dict=feed)\n",
    "\t\tepoch_val_pred.append(val_preds)\n",
    "\t\tepoch_val_label.append(val_y)\n",
    "\t\tepoch_val_loss.append(val_loss)\n",
    "\n",
    "\n",
    "\ttrain_predictions = np.concatenate(epoch_train_pred , axis=0)\n",
    "\ttrain_labels = np.concatenate(epoch_train_label , axis=0)\n",
    "\ttrain_acc = accuracy(labels , predictions) \n",
    "\n",
    "\tval_predictions = np.concatenate(epoch_val_pred , axis=0)\n",
    "\tval_labels = np.concatenate(epoch_val_label , axis=0)\n",
    "\tval_acc = accuracy(labels , predictions) \n",
    "\n",
    "\tprint(\"Training Accuracy: {}\".format(train_acc))\n",
    "\tprint(\"Validation Accuracy: {}\".format(val_acc))\n",
    "\tprint()\n",
    "\n",
    "\treturn epoch_train_loss, epoch_val_loss, train_acc , val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNNModel(filename):\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tconfig = Config()\n",
    "\tmodel = RNNModel(config , filename)\n",
    "\n",
    "\tnum_batches = int(len(model.X_train)/batch_size)\n",
    "\ttrain_loss_history = np.zeros(model.config.num_epochs , model.config.num_batches)\n",
    "\tval_loss_history = np.zeros_like(train_loss_history)\n",
    "\ttrain_acc_history = np.zeros(model.config.num_epochs , model.config.label_size)\n",
    "\tval_acc_history = np.zeros_lke(train_acc_history)\n",
    "\n",
    "\tbest_val_loss = np.float(inf)\n",
    "\tbest_epoch = 0\n",
    "\n",
    "\tif not os.path.exists(\"./weights\"):\n",
    "\t\tos.makedirs(\"./weights\")\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\n",
    "\t\tinit = tf.global_variables_initalizer()\n",
    "\t\tsess.run(init)\n",
    "\n",
    "\t\tfor epoch in num_epochs:\n",
    "\n",
    "\t\t\tepoch_train_loss, epoch_val_loss, epoch_train_acc, epoch_val_acc = run_epoch(sess , model)\n",
    "\t\t\ttrain_acc_history[epoch , :] = epoch_train_acc\n",
    "\t\t\tval_acc_history[epoch , :] = epoch_val_acc\n",
    "\t\t\ttrain_loss_history[epoch , :] = np.array(epoch_train_loss)\n",
    "\t\t\tval_loss_history[epoch , :] = np.array(epoch_val_loss)\n",
    "\n",
    "\t\t\tval_loss = np.mean(epoch_val_loss)\n",
    "\n",
    "\t\t\tif val_loss < best_val_loss:\n",
    "\t\t\t\tbest_val_loss = val_loss\n",
    "\t\t\t\tbest_epoch = epoch\n",
    "\t\t\t\tsaver = tf.train.Saver()\n",
    "\t\t\t\tsaver.save(sess , './weights/%s'%model.config.model_name)\n",
    "\n",
    "\t\t\tif epoch - best_epoch > model.config.anneal_threshold: ## Anneal lr on no improvement in val loss\n",
    "\t\t\t\tmodel.config.lr *= model.config.annealing_factor\n",
    "\n",
    "\t\t\tif epoch - best_epoch > model.config.early_stopping: ## Stop on no improvement\n",
    "\t\t\t\tprint('Stopping due to early stopping')\n",
    "\t\t\t\tbreak;\n",
    "\n",
    "\n",
    "\tprint()\n",
    "\tprint(\"#\"*20)\n",
    "\tprint('Completed Training')\n",
    "\tprint('Training Time:{} minutes'.format((time.time()-start_time)/60))\n",
    "\n",
    "\tplt.plot(np.mean(train_loss_history , axis=0) , linewidth=3 , label='Train')\n",
    "\tplt.plot(np.mean(val_loss_history , axis=0) , linewidth=3 , label='Val')\n",
    "\tplt.xlabel(\"Epoch Number\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.title(\"Loss vs Epoch\")\n",
    "\tplt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RNNModel(filename):\n",
    "\n",
    "\ttest_data = pd.read_csv(filename)\n",
    "\ttest_idx = test_data.iloc[:,0].values\n",
    "\n",
    "\tconfig = Config()\n",
    "\tmodel = RNNModel(config , filename , test=True) ## Builds our model\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsaver = tf.train.import_meta_graph('./weights/%s.meta'%model.config.model_name)\n",
    "\t\tsaver.restore(sess , './weights/%s'%model.config.model_name)\n",
    "\n",
    "\t\tX_test , test_seq_length = get_batches(model.X_test, y=None, batch_size=1)\n",
    "\t\tfeed = model.build_feeddict(X_test , test_seq_length)\n",
    "\t\tpredictions = sess.run([model.pred] , feed_dict=feed)\n",
    "\n",
    "\tassert(len(test_idx) == len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
